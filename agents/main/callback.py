import json
import time
from typing import Any, Dict, List, Optional, Union, cast
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult
from langchain_core.outputs import ChatGenerationChunk, GenerationChunk
from .prompts import fact_check_plan_output_parser, evaluate_search_result_output_parser


class MainAgentCallback(BaseCallbackHandler):
    """
    Callback handler for MainAgent to track and display the agent's reasoning process,
    output tokens, and planning results during execution.
    """

    def __init__(self):
        """
        Initialize the callback handler

        Args:
            verbose: Whether to display detailed information
        """
        self.step_count = 0
        self.llm_call_count = 0
        self.start_time = None
        self.token_usage = 0
        self.has_thinking_started = False
        self.has_content_started = False
        # è·Ÿè¸ªå½“å‰æ˜¯å¦åœ¨ planner graph å†…éƒ¨
        self.is_in_planner_graph = False
        # è·Ÿè¸ªå½“å‰æ­£åœ¨æ‰§è¡Œçš„èŠ‚ç‚¹
        self.current_node = None

        # ANSI color codes
        self.colors = {
            "blue": "\033[94m",
            "green": "\033[92m",
            "yellow": "\033[93m",
            "red": "\033[91m",
            "purple": "\033[95m",
            "cyan": "\033[96m",
            "gray": "\033[90m",
            "bold": "\033[1m",
            "reset": "\033[0m",
        }

    def _print_colored(self, text, color="blue", bold=False):
        """Print colored text"""
        if not self.is_in_planner_graph:
            return

        prefix = ""
        if bold:
            prefix += self.colors["bold"]

        if color in self.colors:
            prefix += self.colors[color]

        print(f"{prefix}{text}{self.colors['reset']}")

    def _format_json(self, data):
        """Format JSON data as readable string"""
        if isinstance(data, str):
            try:
                # Try to parse JSON string
                parsed_data = json.loads(data)
                return json.dumps(parsed_data, indent=2, ensure_ascii=False)
            except:
                return data
        elif isinstance(data, (dict, list)):
            return json.dumps(data, indent=2, ensure_ascii=False)
        else:
            return str(data)

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> None:
        """Called when a chain starts running, check if we're in planner graph"""
        try:            
            # ä»Ž kwargs ä¸­è¯»å– node åç§°
            node_name = None
            if kwargs and "metadata" in kwargs and isinstance(kwargs["metadata"], dict):
                node_name = kwargs["metadata"].get("langgraph_node", "")
            
            # æ£€æŸ¥æ˜¯å¦è¿›å…¥äº†å…¶ä»– graph èŠ‚ç‚¹
            if node_name and ("metadata_extractor" in node_name.lower() or "search_agent" in node_name.lower()):
                self.is_in_planner_graph = False
            else:
                self.is_in_planner_graph = True
        except Exception as e:
            # å‡ºé”™æ—¶ä¿æŒåœ¨ planner graph å†…
            self.is_in_planner_graph = True
            print(f"Error in on_chain_start: {str(e)}")

    def on_chain_end(
        self, outputs: Dict[str, Any], **kwargs: Any
    ) -> None:
        """Called when a chain ends, reset to planner graph context"""
        # é“¾ç»“æŸåŽé‡ç½®ä¸º planner ä¸Šä¸‹æ–‡ï¼Œä½†ä¸é‡ç½® current_node
        self.is_in_planner_graph = True
        
    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:        
        """Called when LLM starts generating"""
        if not self.is_in_planner_graph:
            return

        try:
            self.llm_call_count += 1
            self.start_time = time.time()
            self.has_thinking_started = False
            self.has_content_started = False

            model_name = "Unknown Model"
            if serialized is not None and isinstance(serialized, dict):
                model_name = serialized.get("name", "Unknown Model")

            # æ ¹æ®å½“å‰èŠ‚ç‚¹æ˜¾ç¤ºä¸åŒçš„å¼€å§‹ä¿¡æ¯
            if self.current_node == "evaluate_search_result":
                self._print_colored(
                    f"\nðŸ§  LLM å¼€å§‹è¯„ä¼°æ£€ç´¢ç»“æžœ (è°ƒç”¨ #{self.llm_call_count}, {model_name})",
                    "yellow",
                    True,
                )
            elif self.current_node == "write_fact_checking_report":
                self._print_colored(
                    f"\nðŸ§  LLM å¼€å§‹æ’°å†™æ ¸æŸ¥æŠ¥å‘Š (è°ƒç”¨ #{self.llm_call_count}, {model_name})",
                    "green",
                    True,
                )
            elif self.current_node == "extract_check_point":
                self._print_colored(
                    f"\nðŸ§  LLM å¼€å§‹æå–æ ¸æŸ¥ç‚¹ (è°ƒç”¨ #{self.llm_call_count}, {model_name})",
                    "cyan",
                    True,
                )
            else:
                self._print_colored(
                    f"\nðŸ§  LLM å¼€å§‹æŽ¨ç† (è°ƒç”¨ #{self.llm_call_count}, {model_name})",
                    "purple",
                    True,
                )
        except Exception as e:
            self._print_colored(f"Error in on_llm_start: {str(e)}", "red")

    def on_llm_new_token(
        self,
        token: str,
        *,
        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,
        **kwargs: Any,
    ) -> None:
        """Process streaming tokens from LLM, handling reasoning and content separately"""
        if not self.is_in_planner_graph:
            return

        try:
            if chunk is None:
                print(token, end="", flush=True)
                return

            chunk_message = cast(ChatGenerationChunk, chunk).message
            # Handle reasoning content (thinking process)
            if hasattr(chunk, "message") and hasattr(
                chunk_message, "additional_kwargs"
            ):
                if "reasoning_content" in chunk_message.additional_kwargs:
                    if not self.has_thinking_started:
                        self._print_colored("ðŸ’­ æ€è€ƒ:", "gray", True)
                        self.has_thinking_started = True

                    reasoning = chunk_message.additional_kwargs["reasoning_content"]
                    print(
                        f"{self.colors['gray']}{reasoning}{self.colors['reset']}",
                        end="",
                        flush=True,
                    )

                # Handle regular content (final output)
                elif hasattr(chunk_message, "content") and chunk_message.content:
                    if not self.has_content_started:
                        if self.current_node == "evaluate_search_result":
                            self._print_colored(
                                "\nðŸ”„ æ€è€ƒå®Œæˆï¼ŒLLM æ­£åœ¨è¯„ä¼°æ£€ç´¢ç»“æžœ...", "yellow", True
                            )
                        elif self.current_node == "write_fact_checking_report":
                            self._print_colored(
                                "\nðŸ”„ æ€è€ƒå®Œæˆï¼ŒLLM æ­£åœ¨æ’°å†™æ ¸æŸ¥æŠ¥å‘Š...", "green", True
                            )
                        elif self.current_node == "extract_check_point":
                            self._print_colored(
                                "\nðŸ”„ æ€è€ƒå®Œæˆï¼ŒLLM æ­£åœ¨æå–æ ¸æŸ¥ç‚¹...", "cyan", True
                            )
                        else:
                            self._print_colored(
                                "\nðŸ”„ æ€è€ƒå®Œæˆï¼ŒLLM æ­£åœ¨è§„åˆ’æ ¸æŸ¥æ–¹æ¡ˆ...", "purple", True
                            )
                        self.has_content_started = True
            else:
                # Fallback for simple token streaming
                print(token, end="", flush=True)
        except Exception as e:
            self._print_colored(f"Error in on_llm_new_token: {str(e)}", "red")

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """Called when LLM generation ends"""
        if not self.is_in_planner_graph:
            return

        try:
            # ç”Ÿæˆè€—æ—¶
            if self.start_time:
                generation_time = time.time() - self.start_time
                self._print_colored(f"\nâ±ï¸ æŽ¨ç†è€—æ—¶: {generation_time:.2f}ç§’", "blue")

            # æŽ§åˆ¶å°æ ¼å¼åŒ–è¾“å‡º
            self._print_colored("\nðŸ“‹ è¾“å‡º:", "cyan", True)

            # æ ¹æ®å½“å‰èŠ‚ç‚¹å¤„ç†ä¸åŒçš„è¾“å‡º
            if self.current_node == "evaluate_search_result":
                try:
                    parsed_result = evaluate_search_result_output_parser.parse(
                        response.generations[0][0].text
                    )
                    self._print_verification_results(parsed_result)
                except Exception as e:
                    self._print_colored(f"è§£æžè¯„ä¼°ç»“æžœå¤±è´¥: {str(e)}", "red")
                    print(response.generations[0][0].text)
            elif self.current_node == "write_fact_checking_report":
                # ç›´æŽ¥æ‰“å°æŠ¥å‘Šå†…å®¹ï¼Œä¸å°è¯•è§£æžä¸ºJSON
                report_text = response.generations[0][0].text
                self._print_colored("\nðŸ“Š äº‹å®žæ ¸æŸ¥æŠ¥å‘Š:", "green", True)
                print(report_text)
            elif self.current_node == "extract_check_point":
                # å¤„ç†æ ¸æŸ¥è®¡åˆ’
                try:
                    parsed_result = fact_check_plan_output_parser.parse(
                        response.generations[0][0].text
                    )
                    check_points = parsed_result.items
                    for idx, check_point in enumerate(check_points):
                        print(f"\nç¬¬ {idx+1} æ¡é™ˆè¿°")
                        print(f"é™ˆè¿°å†…å®¹ï¼š{check_point.content}")
                        print(f"æ˜¯å¦éœ€è¦æ ¸æŸ¥ï¼š{check_point.is_verification_point}")
                        print(f"æ ¸æŸ¥ç†ç”±ï¼š{check_point.importance}")
                        if check_point.retrieval_step:
                            for idx, plan in enumerate(check_point.retrieval_step):
                                print(f"æ ¸æŸ¥è®¡åˆ’ {idx+1}ï¼š")
                                print(f"- æ ¸æŸ¥ç›®æ ‡ï¼š{plan.purpose}")
                                print(f"- ç›®æ ‡ä¿¡æºç±»åž‹ï¼š{plan.expected_sources}")
                except Exception as e:
                    self._print_colored(f"è§£æžæ ¸æŸ¥è®¡åˆ’å¤±è´¥: {str(e)}", "red")
                    print(response.generations[0][0].text)
            else:
                # é»˜è®¤æƒ…å†µä¸‹ç›´æŽ¥æ‰“å°è¾“å‡º
                print(response.generations[0][0].text)

        except Exception as e:
            self._print_colored(f"Error in on_llm_end: {str(e)}", "red")

    def _print_verification_results(self, verification_results):
        """æ‰“å°æ£€ç´¢ç»“æžœè¯„ä¼°ä¿¡æ¯"""
        if not isinstance(verification_results, dict) or "items" not in verification_results:
            self._print_colored("æ— æ³•è§£æžè¯„ä¼°ç»“æžœ", "red")
            return
            
        self._print_colored("\nðŸ” æ£€ç´¢ç»“æžœè¯„ä¼°:", "yellow", True)
        
        for item in verification_results["items"]:
            retrieval_step_id = item.get("retrieval_step_id", "æœªçŸ¥ID")
            verified = item.get("verified", False)
            reasoning = item.get("reasoning", "æ— æŽ¨ç†è¿‡ç¨‹")
            
            status_emoji = "âœ…" if verified else "âŒ"
            status_color = "green" if verified else "red"
            
            self._print_colored(f"\n{status_emoji} æ£€ç´¢æ­¥éª¤ ID: {retrieval_step_id}", status_color, True)
            self._print_colored(f"ðŸ“ è¯„ä¼°æŽ¨ç†: {reasoning}", "yellow")
            self._print_colored(f"ðŸ” ç»“è®º: {'è®¤å¯' if verified else 'ä¸è®¤å¯'}", status_color, True)

    def _print_formatted_plan(self, plan_data):
        """Format and print the planning results with appropriate emojis"""
        if not self.is_in_planner_graph:
            return
            
        try:
            if not isinstance(plan_data, dict):
                self._print_colored(str(plan_data), "cyan")
                return

            # Print check points
            if "check_points" in plan_data and isinstance(
                plan_data["check_points"], list
            ):
                for i, point in enumerate(plan_data["check_points"]):
                    is_verification = point.get("is_verification_point", False)
                    emoji = "ðŸ”" if is_verification else "ðŸ“Œ"

                    self._print_colored(
                        f"\n{emoji} é™ˆè¿° #{point.get('id', i+1)}: {point.get('content', 'æ— å†…å®¹')}",
                        "cyan",
                        is_verification,
                    )

                    if is_verification:
                        if "importance" in point and point["importance"]:
                            self._print_colored(
                                f"â­ é‡è¦æ€§: {point['importance']}", "cyan"
                            )

                        if "retrieval_step" in point and point["retrieval_step"]:
                            self._print_colored(f"ðŸ”Ž æ£€ç´¢æ–¹æ¡ˆ:", "cyan")
                            for j, step in enumerate(point["retrieval_step"]):
                                self._print_colored(
                                    f"  {j+1}. ç›®çš„: {step.get('purpose', 'æ— ç›®çš„')}",
                                    "cyan",
                                )
                                if (
                                    "expected_sources" in step
                                    and step["expected_sources"]
                                ):
                                    sources = ", ".join(step["expected_sources"])
                                    self._print_colored(
                                        f"     é¢„æœŸæ¥æº: {sources}", "cyan"
                                    )
                                
                                # æ˜¾ç¤ºæ£€ç´¢ç»“æžœå’ŒéªŒè¯ç»“æžœï¼ˆå¦‚æžœæœ‰ï¼‰
                                if "result" in step and step["result"]:
                                    result = step["result"]
                                    self._print_colored(
                                        f"     ðŸ“Š æ£€ç´¢ç»“è®º: {result.get('conclusion', 'æ— ç»“è®º')}",
                                        "yellow"
                                    )
                                    self._print_colored(
                                        f"     ðŸ” ç½®ä¿¡åº¦: {result.get('confidence', 'æœªçŸ¥')}",
                                        "yellow"
                                    )
                                
                                if "verification" in step and step["verification"]:
                                    verification = step["verification"]
                                    status_emoji = "âœ…" if verification.get("verified", False) else "âŒ"
                                    status_color = "green" if verification.get("verified", False) else "red"
                                    self._print_colored(
                                        f"     {status_emoji} ä¸»æ¨¡åž‹è¯„ä¼°: {'è®¤å¯' if verification.get('verified', False) else 'ä¸è®¤å¯'}",
                                        status_color
                                    )
        except Exception as e:
            self._print_colored(f"Error in _print_formatted_plan: {str(e)}", "red")

    def on_agent_action(self, action, **kwargs: Any) -> Any:
        """Called when agent takes an action"""
        if not self.is_in_planner_graph:
            return

        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯è°ƒç”¨å…¶ä»– agent çš„åŠ¨ä½œ
            tool_name = action.tool.lower() if hasattr(action, "tool") else ""
            if "metadata_extractor" in tool_name or "search" in tool_name:
                self.is_in_planner_graph = False
                self._print_colored(f"\nðŸ”„ è°ƒç”¨å¤–éƒ¨å·¥å…·: {action.tool}", "purple", True)
                return
                
            self._print_colored(f"\nðŸ› ï¸ æ‰§è¡ŒåŠ¨ä½œ: {action.tool}", "purple", True)
            self._print_colored(f"ðŸ“¥ è¾“å…¥: {action.tool_input}", "purple")
            
            # ä»Ž kwargs ä¸­è¯»å– node åç§°
            node_name = None
            if kwargs and "metadata" in kwargs and isinstance(kwargs["metadata"], dict):
                node_name = kwargs["metadata"].get("langgraph_node", "")
            
            # è®¾ç½®å½“å‰èŠ‚ç‚¹
            if node_name == "evaluate_search_result":
                self.current_node = "evaluate_search_result"
            elif node_name == "write_fact_checking_report":
                self.current_node = "write_fact_checking_report"
            elif node_name == "extract_check_point":
                self.current_node = "extract_check_point"
            
        except Exception as e:
            self._print_colored(f"Error in on_agent_action: {str(e)}", "red")

    def on_agent_finish(self, finish, **kwargs: Any) -> None:
        """Called when agent finishes"""
        if not self.is_in_planner_graph:
            return

        try:
            if self.current_node == "evaluate_search_result":
                self._print_colored(f"\nâœ… æ£€ç´¢ç»“æžœè¯„ä¼°å®Œæˆ", "green", True)
            elif self.current_node == "write_fact_checking_report":
                self._print_colored(f"\nâœ… äº‹å®žæ ¸æŸ¥æŠ¥å‘Šç”Ÿæˆå®Œæˆ", "green", True)
            elif self.current_node == "extract_check_point":
                self._print_colored(f"\nâœ… æ ¸æŸ¥ç‚¹æå–å®Œæˆ", "green", True)
            else:
                self._print_colored(f"\nâœ… ä»£ç†å®Œæˆ: {finish.return_values}", "green", True)
        except Exception as e:
            self._print_colored(f"Error in on_agent_finish: {str(e)}", "red")

    def on_tool_error(self, error: BaseException, **kwargs: Any) -> None:
        """Called when a tool errors"""
        if not self.is_in_planner_graph:
            return

        try:
            self._print_colored(f"\nâŒ å·¥å…·æ‰§è¡Œé”™è¯¯:", "red", True)
            self._print_colored(f"{str(error)}", "red")
            self._print_colored(f"{'-'*50}", "red")
        except Exception as e:
            self._print_colored(f"Error in on_tool_error: {str(e)}", "red")

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> None:
        """Called when a tool starts running"""
        try:
            # ä»Ž kwargs ä¸­è¯»å– node åç§°
            node_name = None
            if kwargs and "metadata" in kwargs and isinstance(kwargs["metadata"], dict):
                node_name = kwargs["metadata"].get("langgraph_node", "")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è°ƒç”¨å…¶ä»– agent çš„å·¥å…·
            tool_name = ""
            if serialized is not None and isinstance(serialized, dict):
                tool_name = serialized.get("name", "").lower()
            
            if "metadata_extractor" in tool_name or "search" in tool_name:
                self.is_in_planner_graph = False
            else:
                self.is_in_planner_graph = True
                
            # è®¾ç½®å½“å‰èŠ‚ç‚¹
            if node_name == "evaluate_search_result":
                self.current_node = "evaluate_search_result"
            elif node_name == "write_fact_checking_report":
                self.current_node = "write_fact_checking_report"
            elif node_name == "extract_check_point":
                self.current_node = "extract_check_point"
                
        except Exception as e:
            # å‡ºé”™æ—¶ä¿æŒåœ¨ planner graph å†…
            self.is_in_planner_graph = True
            print(f"Error in on_tool_start: {str(e)}")

    def on_tool_end(
        self, output: str, **kwargs: Any
    ) -> None:
        """Called when a tool ends running"""
        self.is_in_planner_graph = True
