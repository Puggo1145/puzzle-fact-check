import json
import time
from typing import Any, Dict, List, Optional, Union, cast
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult
from langchain_core.outputs import ChatGenerationChunk, GenerationChunk
from .prompts import fact_check_plan_output_parser


class MainAgentCallback(BaseCallbackHandler):
    """
    Callback handler for MainAgent to track and display the agent's reasoning process,
    output tokens, and planning results during execution.
    """

    def __init__(self, verbose=True):
        """
        Initialize the callback handler

        Args:
            verbose: Whether to display detailed information
        """
        self.verbose = verbose
        self.step_count = 0
        self.llm_call_count = 0
        self.start_time = None
        self.token_usage = 0
        self.has_thinking_started = False
        self.has_content_started = False
        # è·Ÿè¸ªå½“å‰æ˜¯å¦åœ¨ planner graph å†…éƒ¨
        self.is_in_planner_graph = True

        # ANSI color codes
        self.colors = {
            "blue": "\033[94m",
            "green": "\033[92m",
            "yellow": "\033[93m",
            "red": "\033[91m",
            "purple": "\033[95m",
            "cyan": "\033[96m",
            "gray": "\033[90m",
            "bold": "\033[1m",
            "reset": "\033[0m",
        }

    def _print_colored(self, text, color="blue", bold=False):
        """Print colored text"""
        if not self.verbose or not self.is_in_planner_graph:
            return

        prefix = ""
        if bold:
            prefix += self.colors["bold"]

        if color in self.colors:
            prefix += self.colors[color]

        print(f"{prefix}{text}{self.colors['reset']}")

    def _format_json(self, data):
        """Format JSON data as readable string"""
        if isinstance(data, str):
            try:
                # Try to parse JSON string
                parsed_data = json.loads(data)
                return json.dumps(parsed_data, indent=2, ensure_ascii=False)
            except:
                return data
        elif isinstance(data, (dict, list)):
            return json.dumps(data, indent=2, ensure_ascii=False)
        else:
            return str(data)

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> None:
        """Called when a chain starts running, check if we're in planner graph"""
        try:
            # æ£€æŸ¥æ˜¯å¦è¿›å…¥äº†å…¶ä»– graph èŠ‚ç‚¹
            chain_name = ""
            if serialized is not None and isinstance(serialized, dict):
                chain_name = serialized.get("name", "")
            
            if "metadata_extractor" in chain_name.lower() or "search_agent" in chain_name.lower():
                self.is_in_planner_graph = False
            else:
                self.is_in_planner_graph = True
        except Exception as e:
            # å‡ºé”™æ—¶ä¿æŒåœ¨ planner graph å†…
            self.is_in_planner_graph = True
            if self.verbose:
                print(f"Error in on_chain_start: {str(e)}")

    def on_chain_end(
        self, outputs: Dict[str, Any], **kwargs: Any
    ) -> None:
        """Called when a chain ends, reset to planner graph context"""
        # é“¾ç»“æŸåé‡ç½®ä¸º planner ä¸Šä¸‹æ–‡
        self.is_in_planner_graph = True

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        """Called when LLM starts generating"""
        if not self.verbose or not self.is_in_planner_graph:
            return

        try:
            self.llm_call_count += 1
            self.start_time = time.time()
            self.has_thinking_started = False
            self.has_content_started = False

            model_name = "Unknown Model"
            if serialized is not None and isinstance(serialized, dict):
                model_name = serialized.get("name", "Unknown Model")

            self._print_colored(
                f"\nğŸ§  LLM å¼€å§‹æ¨ç† (è°ƒç”¨ #{self.llm_call_count}, {model_name})",
                "purple",
                True,
            )
        except Exception as e:
            self._print_colored(f"Error in on_llm_start: {str(e)}", "red")

    def on_llm_new_token(
        self,
        token: str,
        *,
        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,
        **kwargs: Any,
    ) -> None:
        """Process streaming tokens from LLM, handling reasoning and content separately"""
        if not self.verbose or not self.is_in_planner_graph:
            return

        try:
            if chunk is None:
                print(token, end="", flush=True)
                return

            chunk_message = cast(ChatGenerationChunk, chunk).message
            # Handle reasoning content (thinking process)
            if hasattr(chunk, "message") and hasattr(
                chunk_message, "additional_kwargs"
            ):
                if "reasoning_content" in chunk_message.additional_kwargs:
                    if not self.has_thinking_started:
                        self._print_colored("ğŸ’­ æ€è€ƒ:", "gray", True)
                        self.has_thinking_started = True

                    reasoning = chunk_message.additional_kwargs["reasoning_content"]
                    print(
                        f"{self.colors['gray']}{reasoning}{self.colors['reset']}",
                        end="",
                        flush=True,
                    )

                # Handle regular content (final output)
                elif hasattr(chunk_message, "content") and chunk_message.content:
                    if not self.has_content_started:
                        self._print_colored(
                            "\nğŸ”„ æ€è€ƒå®Œæˆï¼ŒLLM æ­£åœ¨è§„åˆ’æ ¸æŸ¥æ–¹æ¡ˆ...", "cyan", True
                        )
                        self.has_content_started = True
            else:
                # Fallback for simple token streaming
                print(token, end="", flush=True)
        except Exception as e:
            self._print_colored(f"Error in on_llm_new_token: {str(e)}", "red")

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """Called when LLM generation ends"""
        if not self.verbose or not self.is_in_planner_graph:
            return

        try:
            # ç”Ÿæˆè€—æ—¶
            if self.start_time:
                generation_time = time.time() - self.start_time
                self._print_colored(f"\nâ±ï¸ æ¨ç†è€—æ—¶: {generation_time:.2f}ç§’", "blue")

            # æ§åˆ¶å°æ ¼å¼åŒ–è¾“å‡º
            self._print_colored("\nğŸ“‹ è§„åˆ’ç»“æœ:", "cyan", True)

            parsed_result = fact_check_plan_output_parser.parse(
                response.generations[0][0].text
            )
            check_points = parsed_result["items"]
            selected_check_points = [
                check_point
                for check_point in check_points
                if check_point["is_verification_point"]
            ]
            for idx, check_point in enumerate(selected_check_points):
                print(f"\nç¬¬ {idx+1} æ¡é™ˆè¿°")
                print(f"é™ˆè¿°å†…å®¹ï¼š{check_point['content']}")
                print(f"æ ¸æŸ¥ç†ç”±ï¼š{check_point['importance']}")
                if isinstance(check_point["retrieval_step"], list):
                    for idx, plan in enumerate(check_point["retrieval_step"]):
                        print(f"æ ¸æŸ¥è®¡åˆ’ {idx+1}ï¼š")
                        print(f"- æ ¸æŸ¥ç›®æ ‡ï¼š{plan['purpose']}")
                        print(f"- ç›®æ ‡ä¿¡æºç±»å‹ï¼š{plan['expected_sources']}")

        except Exception as e:
            self._print_colored(f"Error in on_llm_end: {str(e)}", "red")

    def _print_formatted_plan(self, plan_data):
        """Format and print the planning results with appropriate emojis"""
        if not self.is_in_planner_graph:
            return
            
        try:
            if not isinstance(plan_data, dict):
                self._print_colored(str(plan_data), "cyan")
                return

            # Print check points
            if "check_points" in plan_data and isinstance(
                plan_data["check_points"], list
            ):
                for i, point in enumerate(plan_data["check_points"]):
                    is_verification = point.get("is_verification_point", False)
                    emoji = "ğŸ”" if is_verification else "ğŸ“Œ"

                    self._print_colored(
                        f"\n{emoji} é™ˆè¿° #{point.get('id', i+1)}: {point.get('content', 'æ— å†…å®¹')}",
                        "cyan",
                        is_verification,
                    )

                    if is_verification:
                        if "importance" in point and point["importance"]:
                            self._print_colored(
                                f"â­ é‡è¦æ€§: {point['importance']}", "cyan"
                            )

                        if "retrieval_step" in point and point["retrieval_step"]:
                            self._print_colored(f"ğŸ” æ£€ç´¢æ–¹æ¡ˆ:", "cyan")
                            for j, step in enumerate(point["retrieval_step"]):
                                self._print_colored(
                                    f"  {j+1}. ç›®çš„: {step.get('purpose', 'æ— ç›®çš„')}",
                                    "cyan",
                                )
                                if (
                                    "expected_sources" in step
                                    and step["expected_sources"]
                                ):
                                    sources = ", ".join(step["expected_sources"])
                                    self._print_colored(
                                        f"     é¢„æœŸæ¥æº: {sources}", "cyan"
                                    )
        except Exception as e:
            self._print_colored(f"Error in _print_formatted_plan: {str(e)}", "red")

    def on_agent_action(self, action, **kwargs: Any) -> Any:
        """Called when agent takes an action"""
        if not self.verbose or not self.is_in_planner_graph:
            return

        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯è°ƒç”¨å…¶ä»– agent çš„åŠ¨ä½œ
            tool_name = action.tool.lower() if hasattr(action, "tool") else ""
            if "metadata_extractor" in tool_name or "search" in tool_name:
                self.is_in_planner_graph = False
                self._print_colored(f"\nğŸ”„ è°ƒç”¨å¤–éƒ¨å·¥å…·: {action.tool}", "purple", True)
                return
                
            self._print_colored(f"\nğŸ› ï¸ æ‰§è¡ŒåŠ¨ä½œ: {action.tool}", "purple", True)
            self._print_colored(f"ğŸ“¥ è¾“å…¥: {action.tool_input}", "purple")
        except Exception as e:
            self._print_colored(f"Error in on_agent_action: {str(e)}", "red")

    def on_agent_finish(self, finish, **kwargs: Any) -> None:
        """Called when agent finishes"""
        if not self.verbose or not self.is_in_planner_graph:
            return

        try:
            self._print_colored(f"\nâœ… ä»£ç†å®Œæˆ: {finish.return_values}", "green", True)
        except Exception as e:
            self._print_colored(f"Error in on_agent_finish: {str(e)}", "red")

    def on_tool_error(self, error: BaseException, **kwargs: Any) -> None:
        """Called when a tool errors"""
        if not self.verbose or not self.is_in_planner_graph:
            return

        try:
            self._print_colored(f"\nâŒ å·¥å…·æ‰§è¡Œé”™è¯¯:", "red", True)
            self._print_colored(f"{str(error)}", "red")
            self._print_colored(f"{'-'*50}", "red")
        except Exception as e:
            self._print_colored(f"Error in on_tool_error: {str(e)}", "red")

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> None:
        """Called when a tool starts running"""
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯è°ƒç”¨å…¶ä»– agent çš„å·¥å…·
            tool_name = ""
            if serialized is not None and isinstance(serialized, dict):
                tool_name = serialized.get("name", "").lower()
            
            if "metadata_extractor" in tool_name or "search" in tool_name:
                self.is_in_planner_graph = False
            else:
                self.is_in_planner_graph = True
        except Exception as e:
            # å‡ºé”™æ—¶ä¿æŒåœ¨ planner graph å†…
            self.is_in_planner_graph = True
            if self.verbose:
                print(f"Error in on_tool_start: {str(e)}")

    def on_tool_end(
        self, output: str, **kwargs: Any
    ) -> None:
        """Called when a tool ends running"""
        # å·¥å…·ç»“æŸåé‡ç½®ä¸º planner ä¸Šä¸‹æ–‡
        self.is_in_planner_graph = True
